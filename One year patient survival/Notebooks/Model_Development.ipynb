{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "YEvijD2D_xYI",
        "outputId": "4765dfea-4e4f-4c3d-aefe-39129123219f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing completed. Processed dataset saved as 'processed_dataset.csv'\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://5d052f24cda2f1678a.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://5d052f24cda2f1678a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#STACKED ENSEMBLE WITH SHAP + LIME EXPLAINABILITY + GRADIO INTERFACE - MEMORY OPTIMIZED FOR MAXIMUM FEATURES\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.inspection import permutation_importance\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import gc\n",
        "import psutil\n",
        "import time\n",
        "import gradio as gr\n",
        "import joblib\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"Dataset.csv\")\n",
        "\n",
        "# 1. Drop unnecessary columns\n",
        "df.drop(columns=[\"ID_Patient_Care_Situation\", \"Patient_ID\"], inplace=True)\n",
        "\n",
        "# 2. Binary encode 'Patient_smoker' and 'Patient_rural_urban'\n",
        "df[\"Patient_Smoker\"] = df[\"Patient_Smoker\"].map({\"YES\": 1, \"NO\": 0, \"Cannot say\": None})\n",
        "df[\"Patient_Rural_Urban\"] = df[\"Patient_Rural_Urban\"].map({\"RURAL\": 1, \"URBAN\": 0})\n",
        "\n",
        "# 3. One-hot encode 'Treated_with_drugs'\n",
        "df = df.join(df[\"Treated_with_drugs\"].str.get_dummies(sep=\",\"))\n",
        "df.drop(columns=[\"Treated_with_drugs\"], inplace=True)\n",
        "\n",
        "# 4. Save the processed dataset\n",
        "df.to_csv(\"processed_dataset.csv\", index=False)\n",
        "\n",
        "print(\"Preprocessing completed. Processed dataset saved as 'processed_dataset.csv'\")\n",
        "\n",
        "\n",
        "# Create directory for PDF outputs\n",
        "os.makedirs('explainability_plots_memory_optimized', exist_ok=True)\n",
        "\n",
        "# MEMORY-OPTIMIZED Configuration parameters\n",
        "N_BACKGROUND_SAMPLES = 100  # Reduced from 50 to save memory\n",
        "N_TEST_SAMPLES = 20000       # Reduced from 2000 but still substantial\n",
        "SHAP_NSAMPLES = 200        # Reduced from 100 for faster computation\n",
        "BATCH_SIZE = 50           # Process SHAP values in batches\n",
        "MAX_FEATURES_DISPLAY = 46  # Show more features in plots\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"MEMORY-OPTIMIZED SHAP ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "print(\"Configuration:\")\n",
        "print(f\"- Background samples: {N_BACKGROUND_SAMPLES}\")\n",
        "print(f\"- Test samples: {N_TEST_SAMPLES}\")\n",
        "print(f\"- SHAP nsamples: {SHAP_NSAMPLES}\")\n",
        "print(f\"- Batch processing: {BATCH_SIZE}\")\n",
        "print(f\"- Features to display: {MAX_FEATURES_DISPLAY}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def get_memory_usage():\n",
        "    \"\"\"Get current memory usage in MB\"\"\"\n",
        "    process = psutil.Process(os.getpid())\n",
        "    return process.memory_info().rss / 1024 / 1024\n",
        "\n",
        "def clear_memory():\n",
        "    \"\"\"Force garbage collection to free memory\"\"\"\n",
        "    gc.collect()\n",
        "\n",
        "print(f\"Initial memory usage: {get_memory_usage():.1f} MB\")\n",
        "\n",
        "# Install required packages\n",
        "def install_package(package):\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "shap_installed = install_package(\"shap\")\n",
        "lime_installed = install_package(\"lime\")\n",
        "\n",
        "if shap_installed:\n",
        "    import shap\n",
        "if lime_installed:\n",
        "    import lime\n",
        "    import lime.lime_tabular\n",
        "    from lime.lime_tabular import LimeTabularExplainer\n",
        "\n",
        "# Load dataset with memory optimization\n",
        "print(\"Loading dataset with memory optimization...\")\n",
        "df = pd.read_csv(\"processed_dataset.csv\")\n",
        "df = df.dropna(subset=['Patient_Smoker'])\n",
        "df = df.fillna(0)\n",
        "\n",
        "# Convert to more memory-efficient dtypes\n",
        "print(\"Optimizing data types...\")\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == 'float64':\n",
        "        df[col] = df[col].astype('float32')\n",
        "    elif df[col].dtype == 'int64':\n",
        "        df[col] = df[col].astype('int32')\n",
        "\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Memory usage after optimization: {get_memory_usage():.1f} MB\")\n",
        "\n",
        "X = df.drop(\"Survived_1_year\", axis=1)\n",
        "y = df[\"Survived_1_year\"]\n",
        "feature_names = X.columns.tolist()\n",
        "\n",
        "print(f\"Total features: {len(feature_names)}\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Use float32 for scaling to save memory\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train).astype(np.float32)\n",
        "X_test_scaled = scaler.transform(X_test).astype(np.float32)\n",
        "\n",
        "print(f\"Memory after scaling: {get_memory_usage():.1f} MB\")\n",
        "clear_memory()\n",
        "\n",
        "# Build model\n",
        "print(\"Training stacked ensemble...\")\n",
        "base_models = [\n",
        "    ('dt', DecisionTreeClassifier(random_state=42)),\n",
        "    ('lr', LogisticRegression(max_iter=1000, random_state=42)),\n",
        "    ('nb', BernoulliNB()),\n",
        "    ('rf', RandomForestClassifier(n_estimators=50, random_state=42)),  # Reduced trees\n",
        "    ('xgb', XGBClassifier(eval_metric='logloss', n_estimators=50, random_state=42)),  # Reduced trees\n",
        "    ('knn', KNeighborsClassifier(n_neighbors=3))  # Reduced neighbors\n",
        "]\n",
        "\n",
        "meta_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "stacked_clf = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=3, n_jobs=-1)\n",
        "stacked_clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred = stacked_clf.predict(X_test_scaled)\n",
        "y_pred_proba = stacked_clf.predict_proba(X_test_scaled)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Model Performance:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix - Memory Optimized Ensemble\")\n",
        "plt.savefig('explainability_plots_memory_optimized/confusion_matrix.pdf', bbox_inches='tight', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# MEMORY-OPTIMIZED SHAP Analysis\n",
        "if shap_installed:\n",
        "    try:\n",
        "        print(f\"\\nMemory before SHAP: {get_memory_usage():.1f} MB\")\n",
        "\n",
        "        # Generate smaller but representative background samples\n",
        "        print(f\"Generating {N_BACKGROUND_SAMPLES} background samples using K-means...\")\n",
        "        background_samples = shap.kmeans(X_train_scaled, N_BACKGROUND_SAMPLES)\n",
        "        print(f\"Background samples shape: {background_samples.data.shape}\")\n",
        "\n",
        "        # Generate representative test samples (smaller batch)\n",
        "        actual_test_samples = min(N_TEST_SAMPLES, len(X_test_scaled))\n",
        "        print(f\"Sampling {actual_test_samples} test samples...\")\n",
        "        X_test_sample = shap.sample(X_test_scaled, actual_test_samples, random_state=42)\n",
        "        print(f\"Test sample shape: {X_test_sample.shape}\")\n",
        "\n",
        "        clear_memory()\n",
        "        print(f\"Memory after sampling: {get_memory_usage():.1f} MB\")\n",
        "\n",
        "        # Model prediction wrapper\n",
        "        def model_predict(X):\n",
        "            return stacked_clf.predict_proba(X)[:, 1]\n",
        "\n",
        "        # Create explainer\n",
        "        print(\"Creating SHAP KernelExplainer...\")\n",
        "        explainer = shap.KernelExplainer(model_predict, background_samples)\n",
        "\n",
        "        # BATCH PROCESSING for SHAP values to manage memory\n",
        "        print(f\"Computing SHAP values in batches of {BATCH_SIZE}...\")\n",
        "\n",
        "        all_shap_values = []\n",
        "        num_batches = (len(X_test_sample) + BATCH_SIZE - 1) // BATCH_SIZE\n",
        "\n",
        "        for batch_idx in range(num_batches):\n",
        "            start_idx = batch_idx * BATCH_SIZE\n",
        "            end_idx = min((batch_idx + 1) * BATCH_SIZE, len(X_test_sample))\n",
        "\n",
        "            print(f\"Processing batch {batch_idx + 1}/{num_batches} (samples {start_idx}-{end_idx})...\")\n",
        "            batch_data = X_test_sample[start_idx:end_idx]\n",
        "\n",
        "            batch_shap_values = explainer.shap_values(batch_data, nsamples=SHAP_NSAMPLES)\n",
        "            all_shap_values.append(batch_shap_values)\n",
        "\n",
        "            # Clear memory after each batch\n",
        "            clear_memory()\n",
        "            print(f\"Memory after batch {batch_idx + 1}: {get_memory_usage():.1f} MB\")\n",
        "\n",
        "        # Combine all SHAP values\n",
        "        print(\"Combining SHAP values from all batches...\")\n",
        "        shap_values = np.vstack(all_shap_values)\n",
        "        print(f\"Final SHAP values shape: {shap_values.shape}\")\n",
        "\n",
        "        # Clear intermediate results\n",
        "        del all_shap_values\n",
        "        clear_memory()\n",
        "\n",
        "        # Save SHAP values to avoid recomputation\n",
        "        print(\"Saving SHAP values...\")\n",
        "        np.save('explainability_plots_memory_optimized/shap_values.npy', shap_values)\n",
        "        np.save('explainability_plots_memory_optimized/X_test_sample.npy', X_test_sample)\n",
        "\n",
        "        # SHAP Feature Importance Plot - Show MORE features\n",
        "        print(f\"Creating SHAP feature importance plot ({MAX_FEATURES_DISPLAY} features)...\")\n",
        "        plt.figure(figsize=(12, MAX_FEATURES_DISPLAY * 0.4))  # Dynamic height\n",
        "        shap.summary_plot(shap_values, X_test_sample, feature_names=feature_names,\n",
        "                         plot_type=\"bar\", show=False, max_display=MAX_FEATURES_DISPLAY)\n",
        "        plt.title(f\"SHAP Feature Importance - Top {MAX_FEATURES_DISPLAY} Features\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('explainability_plots_memory_optimized/shap_feature_importance.pdf', bbox_inches='tight', dpi=300)\n",
        "        plt.show()\n",
        "\n",
        "        # SHAP Summary Plot - Show MORE features\n",
        "        print(f\"Creating SHAP summary plot ({MAX_FEATURES_DISPLAY} features)...\")\n",
        "        plt.figure(figsize=(12, MAX_FEATURES_DISPLAY * 0.4))\n",
        "        shap.summary_plot(shap_values, X_test_sample, feature_names=feature_names,\n",
        "                         show=False, max_display=MAX_FEATURES_DISPLAY)\n",
        "        plt.title(f\"SHAP Summary Plot - Top {MAX_FEATURES_DISPLAY} Features\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('explainability_plots_memory_optimized/shap_summary_plot.pdf', bbox_inches='tight', dpi=300)\n",
        "        plt.show()\n",
        "\n",
        "        # SHAP Waterfall Plot\n",
        "        print(\"Creating SHAP waterfall plot...\")\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        shap.waterfall_plot(shap.Explanation(values=shap_values[0],\n",
        "                                           base_values=explainer.expected_value,\n",
        "                                           data=X_test_sample[0],\n",
        "                                           feature_names=feature_names),\n",
        "                           max_display=15, show=False)  # Show top 15 in waterfall\n",
        "        plt.title(\"SHAP Waterfall Plot - Instance 1\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('explainability_plots_memory_optimized/shap_waterfall_plot.pdf', bbox_inches='tight', dpi=300)\n",
        "        plt.show()\n",
        "\n",
        "        # Enhanced Global Feature Importance Analysis\n",
        "        print(\"Computing comprehensive feature importance...\")\n",
        "        shap_importance = np.abs(shap_values).mean(0)\n",
        "        shap_std = np.abs(shap_values).std(0)\n",
        "\n",
        "        shap_feature_importance = pd.DataFrame({\n",
        "            'feature': feature_names,\n",
        "            'mean_importance': shap_importance,\n",
        "            'std_importance': shap_std,\n",
        "            'cv': shap_std / (shap_importance + 1e-8)\n",
        "        }).sort_values('mean_importance', ascending=False)\n",
        "\n",
        "        # Save detailed results\n",
        "        shap_feature_importance.to_csv('explainability_plots_memory_optimized/feature_importance_detailed.csv', index=False)\n",
        "\n",
        "        # Enhanced Global Feature Importance Plot\n",
        "        print(f\"Creating enhanced global feature importance plot ({MAX_FEATURES_DISPLAY} features)...\")\n",
        "        plt.figure(figsize=(14, MAX_FEATURES_DISPLAY * 0.4))\n",
        "        top_features = shap_feature_importance.head(MAX_FEATURES_DISPLAY)\n",
        "\n",
        "        bars = plt.barh(range(len(top_features)), top_features['mean_importance'])\n",
        "        plt.yticks(range(len(top_features)), top_features['feature'])\n",
        "        plt.xlabel('Mean |SHAP Value|')\n",
        "        plt.title(f'Global Feature Importance - Top {MAX_FEATURES_DISPLAY} Features (Memory Optimized)')\n",
        "        plt.gca().invert_yaxis()\n",
        "\n",
        "        # Add error bars\n",
        "        plt.errorbar(top_features['mean_importance'], range(len(top_features)),\n",
        "                    xerr=top_features['std_importance'], fmt='none', color='red', alpha=0.5)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('explainability_plots_memory_optimized/global_feature_importance_shap.pdf', bbox_inches='tight', dpi=300)\n",
        "        plt.show()\n",
        "\n",
        "        # Feature Importance Tiers Analysis\n",
        "        print(\"Creating feature importance tiers...\")\n",
        "        plt.figure(figsize=(14, 8))\n",
        "\n",
        "        # Create tiers\n",
        "        n_features = len(shap_feature_importance)\n",
        "        tier1 = shap_feature_importance.head(10)\n",
        "        tier2 = shap_feature_importance.iloc[10:25]\n",
        "        tier3 = shap_feature_importance.iloc[25:50] if n_features > 25 else pd.DataFrame()\n",
        "\n",
        "        x_pos = 0\n",
        "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
        "\n",
        "        for i, (tier, label, color) in enumerate([(tier1, 'Tier 1 (Top 10)', colors[0]),\n",
        "                                                  (tier2, 'Tier 2 (11-25)', colors[1]),\n",
        "                                                  (tier3, 'Tier 3 (26-50)', colors[2])]):\n",
        "            if len(tier) > 0:\n",
        "                plt.barh(range(x_pos, x_pos + len(tier)), tier['mean_importance'],\n",
        "                        color=color, alpha=0.7, label=label)\n",
        "                plt.yticks(range(x_pos, x_pos + len(tier)), tier['feature'], fontsize=8)\n",
        "                x_pos += len(tier)\n",
        "\n",
        "        plt.xlabel('Mean |SHAP Value|')\n",
        "        plt.title('SHAP Feature Importance by Tiers')\n",
        "        plt.legend()\n",
        "        plt.gca().invert_yaxis()\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('explainability_plots_memory_optimized/feature_importance_tiers.pdf', bbox_inches='tight', dpi=300)\n",
        "        plt.show()\n",
        "\n",
        "        print(\"SHAP analysis completed successfully!\")\n",
        "        print(f\"Final memory usage: {get_memory_usage():.1f} MB\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"SHAP failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        shap_installed = False\n",
        "\n",
        "# LIME Analysis with more features\n",
        "if lime_installed:\n",
        "    try:\n",
        "        print(\"\\nRunning LIME analysis...\")\n",
        "        lime_explainer = LimeTabularExplainer(\n",
        "            X_train_scaled,\n",
        "            feature_names=feature_names,\n",
        "            class_names=['Did not survive', 'Survived'],\n",
        "            mode='classification',\n",
        "            discretize_continuous=True\n",
        "        )\n",
        "\n",
        "        def predict_fn(X):\n",
        "            return stacked_clf.predict_proba(X)\n",
        "\n",
        "        for i in range(3):\n",
        "            instance = X_test_scaled[i]\n",
        "            exp = lime_explainer.explain_instance(\n",
        "                instance,\n",
        "                predict_fn,\n",
        "                num_features=15,  # Show more features\n",
        "                top_labels=2\n",
        "            )\n",
        "\n",
        "            fig = exp.as_pyplot_figure(label=1)\n",
        "            plt.title(f\"LIME Explanation - Instance {i+1} (15 Features)\")\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f'explainability_plots_memory_optimized/lime_explanation_instance_{i+1}.pdf', bbox_inches='tight', dpi=300)\n",
        "            plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"LIME failed: {e}\")\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MEMORY-OPTIMIZED ANALYSIS COMPLETED!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if shap_installed and 'shap_feature_importance' in locals():\n",
        "    print(f\"Top {min(20, len(shap_feature_importance))} Most Important Features:\")\n",
        "    display_features = min(20, len(shap_feature_importance))\n",
        "    print(shap_feature_importance.head(display_features)[['feature', 'mean_importance']].to_string(index=False))\n",
        "\n",
        "print(f\"\\nMemory optimization results:\")\n",
        "print(f\"- Analyzed {MAX_FEATURES_DISPLAY} features in detail\")\n",
        "print(f\"- Used {N_TEST_SAMPLES} test samples\")\n",
        "print(f\"- Batch processing prevented memory crashes\")\n",
        "print(f\"- Final memory usage: {get_memory_usage():.1f} MB\")\n",
        "print(f\"\\nAll results saved to 'explainability_plots_memory_optimized' directory\")\n",
        "\n",
        "# ----------------------\n",
        "# Train full model for Gradio Interface\n",
        "# ----------------------\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING FULL MODEL FOR GRADIO INTERFACE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Reload and prepare full dataset for interface\n",
        "df_full = pd.read_csv(\"processed_dataset.csv\")\n",
        "df_full = df_full.dropna(subset=['Patient_Smoker'])\n",
        "df_full = df_full.fillna(0)\n",
        "df_full = pd.get_dummies(df_full, drop_first=True)\n",
        "\n",
        "X_full = df_full.drop(\"Survived_1_year\", axis=1)\n",
        "y_full = df_full[\"Survived_1_year\"]\n",
        "\n",
        "# Scale full dataset\n",
        "scaler_full = StandardScaler()\n",
        "X_full_scaled = scaler_full.fit_transform(X_full)\n",
        "\n",
        "# Train on full data for interface\n",
        "print(\"Training stacked ensemble on full dataset for interface...\")\n",
        "base_models_full = [\n",
        "    ('dt', DecisionTreeClassifier(random_state=42)),\n",
        "    ('lr', LogisticRegression(max_iter=1000, random_state=42)),\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "    ('xgb', XGBClassifier(eval_metric='logloss', n_estimators=100, random_state=42)),\n",
        "    ('knn', KNeighborsClassifier())\n",
        "]\n",
        "\n",
        "meta_model_full = LogisticRegression(max_iter=1000, random_state=42)\n",
        "stacked_clf_full = StackingClassifier(estimators=base_models_full, final_estimator=meta_model_full, cv=3, n_jobs=-1)\n",
        "stacked_clf_full.fit(X_full_scaled, y_full)\n",
        "\n",
        "print(\"Full model training completed!\")\n",
        "\n",
        "# ----------------------\n",
        "# Gradio prediction function\n",
        "# ----------------------\n",
        "def predict_survival(age, bmi, smoker, rural_urban, prev_conditions):\n",
        "    # Convert categorical inputs\n",
        "    smoker_val = 1 if smoker == \"YES\" else 0\n",
        "    rural_val = 1 if rural_urban == \"RURAL\" else 0\n",
        "\n",
        "    # Create a dataframe row for input\n",
        "    input_data = pd.DataFrame([[age, bmi, smoker_val, rural_val, prev_conditions]],\n",
        "                              columns=[\"Patient_Age\", \"Patient_Body_Mass_Index\",\n",
        "                                       \"Patient_Smoker\", \"Patient_Rural_Urban\",\n",
        "                                       \"Number_of_prev_cond\"])\n",
        "\n",
        "    # Align columns with training data\n",
        "    input_data = pd.get_dummies(input_data, drop_first=True).reindex(columns=X_full.columns, fill_value=0)\n",
        "\n",
        "    # Scale\n",
        "    input_scaled = scaler_full.transform(input_data)\n",
        "\n",
        "    # Prediction\n",
        "    pred = stacked_clf_full.predict(input_scaled)[0]\n",
        "    prob = stacked_clf_full.predict_proba(input_scaled)[0][1]\n",
        "\n",
        "    result = \"Likely to Survive (1 year)\" if pred == 1 else \"Not Likely to Survive (1 year)\"\n",
        "    return result, f\"Survival Probability: {prob:.2f}\"\n",
        "\n",
        "# ----------------------\n",
        "# Build Gradio UI (Full-Screen Layout)\n",
        "# ----------------------\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"LAUNCHING GRADIO INTERFACE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "with gr.Blocks(css=\"\"\"\n",
        "    .gradio-container {max-width: 100% !important;}\n",
        "    footer {display: none !important;}\n",
        "\"\"\") as demo:\n",
        "    gr.Markdown(\"## Patient Survival Prediction (1 Year)\")\n",
        "\n",
        "    with gr.Row():\n",
        "        age = gr.Number(label=\"Patient Age\")\n",
        "        bmi = gr.Number(label=\"Patient BMI\")\n",
        "        smoker = gr.Radio([\"YES\", \"NO\"], label=\"Smoker\")\n",
        "        rural_urban = gr.Radio([\"URBAN\", \"RURAL\"], label=\"Rural/Urban\")\n",
        "        prev_conditions = gr.Number(label=\"Previous Conditions\")\n",
        "\n",
        "    with gr.Row():\n",
        "        output = gr.Label(label=\"Prediction\")\n",
        "        output_prob = gr.Textbox(label=\"Probability Score\")\n",
        "\n",
        "    btn = gr.Button(\"Predict Survival\")\n",
        "    btn.click(\n",
        "        predict_survival,\n",
        "        inputs=[age, bmi, smoker, rural_urban, prev_conditions],\n",
        "        outputs=[output, output_prob]\n",
        "    )\n",
        "\n",
        "demo.launch()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
